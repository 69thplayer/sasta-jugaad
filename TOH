                                                                                              PRACTICAL 1 - HYPOTHESIS TESTING FOR MEAN
                                                                                    (A) Perform testing of hypothesis using One sample t – test.


import numpy as np
from scipy import stats

sample_data = [68, 72, 75, 69, 71, 74, 70, 73, 76, 69]
population_mean = 70

t_statistic, p_value = stats.ttest_1samp(sample_data, population_mean)

print(f"T-statistic: {t_statistic}")
print(f"P-value: {p_value}")

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The sample mean is significantly different from the population mean.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference from the population mean.")



                                                                                    (B) Perform testing of hypothesis using Two sample t – test.


import numpy as np
from scipy import stats

class1_scores = [85, 88, 90, 87, 85, 84, 91, 86, 89, 87]
class2_scores = [78, 75, 79, 82, 80, 77, 76, 81, 79, 80]

t_statistic, p_value = stats.ttest_ind(class1_scores, class2_scores)

print(f"T-statistic: {t_statistic}")
print(f"P-value: {p_value}")

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The means of the two groups are significantly different.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference between the means of the two groups.")



                                                                                      (C) Perform testing of hypothesis using z – test.



import numpy as np
from scipy import stats

sample_data = [72, 74, 69, 80, 77, 75, 73, 78, 76, 79, 71, 74, 72, 80, 74, 76, 75, 79, 70, 73, 
               78, 76, 71, 79, 75, 74, 77, 72, 74, 76, 73, 70, 72, 74, 79, 80, 77, 75, 74, 73, 
               72, 76, 78, 74, 75, 79, 77, 72, 80, 73]

population_mean = 75
population_std_dev = 10
sample_size = len(sample_data)
sample_mean = np.mean(sample_data)

z_score = (sample_mean - population_mean) / (population_std_dev / np.sqrt(sample_size))
p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

print(f"Z-score: {z_score}")
print(f"P-value: {p_value}")

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The sample mean is significantly different from the population mean.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference from the population mean.")



                                                                                                            practical 2 -Goodness-of-fit test

                                                                                                (A) Perform goodness-of-fit test using chi-squared test.



from scipy.stats import chisquare

observed = [50, 30, 40]
expected = [40, 40, 40]

chi2_statistic, p_value = chisquare(f_obs=observed, f_exp=expected)

print(f"Chi-Square Statistic: {chi2_statistic}")
print(f"P-value: {p_value}")

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The observed frequencies do not match the expected frequencies.")
else:
    print("Fail to reject the null hypothesis: The observed frequencies match the expected frequencies.")



                                                                                    (B) Perform testing of hypothesis using chi-squared Test of INDEPENDENCE




import numpy as np
from scipy.stats import chi2_contingency

data = np.array([
    [60, 40],  # High School
    [80, 20],  # Bachelor's
    [90, 10]   # Master's
])

chi2_statistic, p_value, dof, expected = chi2_contingency(data)

print("Chi-Square Statistic:", chi2_statistic)
print("P-value:", p_value)
print("Degrees of Freedom:", dof)
print("Expected Frequencies:")
print(expected)

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The variables are not independent.")
else:
    print("Fail to reject the null hypothesis: The variables are independent.")



                                                                                                  (C) Perform goodness-of-fit test using KS test.


import numpy as np
from scipy.stats import kstest, norm

data = np.random.normal(loc=0, scale=1, size=100)

ks_statistic, p_value = kstest(data, 'norm', args=(0, 1))

print(f"KS Statistic: {ks_statistic}")
print(f"P-value: {p_value}")

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The data does not follow the normal distribution.")
else:
    print("Fail to reject the null hypothesis: The data follows the normal distribution.")




                                                                                                          PRACTICAL 3 - Variance Testing
  
                                                                                                    (A) Perform using F-tests of two variances



import numpy as np
from scipy import stats

datal = [12, 15, 14, 10, 13, 18, 16, 19, 11, 17]
data2 = [22, 25, 28, 21, 27, 23, 30, 22, 26, 29]

varl = np.var(datal, ddof=1)
var2 = np.var(data2, ddof=1)

f_statistic = varl / var2 if varl > var2 else var2 / varl

df1 = len(datal) - 1
df2 = len(data2) - 1

p_value = stats.f.sf(f_statistic, df1, df2)

print(f"Variance of datal: {varl}")
print(f"Variance of data2: {var2}")
print(f"F-statistic: {f_statistic}")
print(f"Degrees of freedom: df1 = {df1}, df2 = {df2}")
print(f"P-value: {p_value}")

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The variances are significantly different.")
else:
    print("Fail to reject the null hypothesis: The variances are not significantly different.")



                                                                                                        (B) Perform Testing of Homogeneity



import numpy as np
from scipy import stats

data = np.array([
    [20, 30, 50],  # Population 1 (Counts in categories A, B, C)
    [30, 40, 301]  # Population 2 (Counts in categories A, B, C)
])

chi2_stat, p_value, dof, expected = stats.chi2_contingency(data)

print(f"Chi-square statistic: {chi2_stat}")
print(f"P-value: {p_value}")
print(f"Degrees of freedom: {dof}")
print(f"Expected frequencies:\n{expected}")

alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: The populations do not have the same distribution.")
else:
    print("Fail to reject the null hypothesis: The populations have the same distribution.")




                                                                                              practical 4- ANALYSIS OF VARIANCE AND COVARIANCE

                                                                                            (A) Perform testing of hypothesis using one-way ANOVA



import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols

data = {
    'Method': ['A'] * 5 + ['B'] * 5 + ['C'] * 5,
    'Score': [85, 88, 90, 92, 87, 75, 78, 80, 79, 77, 65, 68, 70, 72, 69]
}

df = pd.DataFrame(data)

model = ols('Score ~ C(Method)', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

print(anova_table)

# Use .iloc[] for positional indexing
p_value = anova_table["PR(>F)"].iloc[0]  
alpha = 0.05

if p_value < alpha:
    print("Reject Ho: There is a significant difference between groups.")
else:
    print("Fail to Reject Ho: No significant difference between groups.")



                                                                                        (B) Perform testing of hypothesis using Two-way ANOVA



import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols

data = {
    'Method': ['A'] * 6 + ['B'] * 6,
    'Gender': ['M', 'F'] * 6,
    'Score': [85, 80, 78, 90, 88, 87, 75, 70, 68, 79, 77, 76]
}

df = pd.DataFrame(data)

model = ols('Score ~ C(Method) + C(Gender) + C(Method):C(Gender)', data=df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

print(anova_table)

p_values = anova_table["PR(>F)"]
alpha = 0.05

for factor, p in p_values.items():
    if p < alpha:
        print(f"Reject Ho for {factor}: Significant effect detected.")
    else:
        print(f"Fail to Reject Ho for {factor}: No significant effect detected.")



                                                                                          (C) Perform testing of hypothesis using MANOVA



import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.multivariate.manova import MANOVA

data = {
    'Diet': ['A'] * 5 + ['B'] * 5 + ['C'] * 5,
    'Weightloss': [3, 2.5, 3.2, 3.8, 3.1, 5, 5.5, 5.2, 4.8, 5.3, 7, 7.5, 7.2, 7.8, 7.3],
    'SugarReduction': [10, 12, 11, 13, 14, 18, 19, 17, 20, 18, 25, 27, 26, 28, 30]
}

df = pd.DataFrame(data)

manova = MANOVA.from_formula('Weightloss + SugarReduction ~ Diet', data=df)
result = manova.mv_test()

print(result)

p_value = result.results['Diet']['stat']['Pr > F'].iloc[0]
alpha = 0.05

if p_value < alpha:
    print("Reject Ho: Diet has a significant effect on at least one dependent variable.")
else:
    print("Fail to Reject Ho: No significant effect of Diet on the dependent variables.")



                                                                                                PRACTICAL 5 - Regression

                                                                                          (A) Perform Simple Linear Regression



import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1) 
Y = np.array([2, 4, 5, 4, 5]) 

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print(f"Intercept: {model.intercept_}")
print(f"Coefficient: {model.coef_[0]}")

mse = mean_squared_error(Y_test, Y_pred)
print(f"Mean Squared Error: {mse}")

plt.scatter(X, Y, color='blue', label='Data points')
plt.plot(X, model.predict(X), color='red', label='Regression Line')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()




                                                                                                  (B) Perform Multiple Linear Regression



import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  
Y = np.array([2, 4, 5, 4, 5])  

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print(f"Intercept: {model.intercept_}")
print(f"Coefficients: {model.coef_}")

mse = mean_squared_error(Y_test, Y_pred)
print(f"Mean Squared Error: {mse}")




                                                                                                    (C) Perform Polynomial Regression



import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Feature variable
Y = np.array([1, 4, 9, 16, 25])  # Target variable (Y = X^2)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

poly = PolynomialFeatures(degree=2)
X_poly_train = poly.fit_transform(X_train)  # Transforming the data into polynomial features

model = LinearRegression()
model.fit(X_poly_train, Y_train)

X_poly_test = poly.transform(X_test)  
Y_pred = model.predict(X_poly_test)

print(f"Intercept: {model.intercept_}")
print(f"Coefficients: {model.coef_}")

mse = mean_squared_error(Y_test, Y_pred)
print(f"Mean Squared Error: {mse}")

plt.scatter(X, Y, color='blue', label='Data points')
plt.plot(X, model.predict(poly.transform(X)), color='red', label='Polynomial Regression Curve')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()




                                                                                      PRACTICAL 6- Perform spatial series and spatial auto-regression


import numpy as np
import pandas as pd
import libpysal
from spreg import OLS, GM_Lag
import matplotlib.pyplot as plt

data = {
    'id': [1, 2, 3, 4, 5],
    'lat': [34.05, 34.07, 34.08, 34.10, 34.12],
    'lon': [-118.25, -118.27, -118.29, -118.30, -118.32],
    'value': [2.5, 3.6, 4.2, 5.0, 5.4]
}
df = pd.DataFrame(data)

coords = np.array(df[['lat', 'lon']])

w = libpysal.weights.KNN.from_array(coords, k=2)
w.transform = 'r'  

plt.figure(figsize=(10, 6))
plt.scatter(df['lon'], df['lat'], c=df['value'], cmap='viridis', s=100)
plt.colorbar(label='Value')
plt.title('Spatial Data Visualization')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.tight_layout()
plt.show()

y = df['value'].values  
X = np.ones((df.shape[0], 1)) 

X = np.column_stack((X, df['lon'].values))

sar_model = GM_Lag(y, X, w=w)
print("Spatial Autoregression Results (SAR - GM Lag):")
print(sar_model.summary)

ols_model = OLS(y, X)
print("\nOLS Results for Comparison:")
print(ols_model.summary)




                                                                                PRACTICAL 7 - Perform time series analysis using Moving averages
  


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 1. Generate a sample time series data
np.random.seed(0)
date_range = pd.date_range(start="2025-01-01", periods=20, freq='D')
data = np.random.randn(20) * 10 + 50  # Generate random data with a mean of 50 and std dev of 10

# Create a DataFrame
df = pd.DataFrame(data, columns=["Value"], index=date_range)

# 2. Calculate moving averages
df['SMA 3'] = df['Value'].rolling(window=3).mean()  # 3-day Simple Moving Average
df['SMA 7'] = df['Value'].rolling(window=7).mean()  # 7-day Simple Moving Average

# 3. Plot the results
plt.figure(figsize=(10, 6))
plt.plot(df.index, df['Value'], label='Original Time Series', marker='o', linestyle='-', color='blue')
plt.plot(df.index, df['SMA 3'], label='3-Day Moving Average', marker='x', linestyle='--', color='red')
plt.plot(df.index, df['SMA 7'], label='7-Day Moving Average', marker='s', linestyle='-.', color='green')

# Adding titles and labels
plt.title('Time Series with Moving Averages')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

# Show the plot
plt.show()




                                                                                  practical 8 - Perform time series analysis using Trend Analysis



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generating some example time series data (for example, monthly data)
data = {
    'Date': pd.date_range(start='2020-01-01', periods=12, freq='ME'),
    'Value': [10, 12, 14, 16, 18, 20, 21, 23, 25, 26, 28, 30]  # Example increasing trend
}

# Convert data into a pandas DataFrame
df = pd.DataFrame(data)
df.set_index('Date', inplace=True)

# Visualize the original data
plt.figure(figsize=(10, 5))
plt.plot(df.index, df['Value'], label='Original Data', color='blue')
plt.title("Time Series Data")
plt.xlabel("Date")
plt.ylabel("Value")
plt.legend()
plt.show()

# Convert time series index into numerical values for regression (ordinal time)
df['Time'] = np.arange(len(df))

# Fit linear regression model
X = df[['Time']]  # Feature: time
y = df['Value']  # Target: value

model = LinearRegression()
model.fit(X, y)

# Predict the trend (linear regression line)
df['Trend'] = model.predict(X)

# Visualize the trend with the original data
plt.figure(figsize=(10, 5))
plt.plot(df.index, df['Value'], label='Original Data', color='blue')
plt.plot(df.index, df['Trend'], label='Trend Line', color='red', linestyle='--')
plt.title("Time Series with Trend Analysis")
plt.xlabel("Date")
plt.ylabel("Value")
plt.legend()
plt.show()

# Output the model parameters (slope and intercept)
print(f"Slope (Trend): {model.coef_[0]}")
print(f"Intercept: {model.intercept_}")

# Displaying the trend and actual data in the DataFrame
print(df[['Value', 'Trend']])




                                                                                                PRACTICAL 9 - Perform Spectral Analysis



import numpy as np
import matplotlib.pyplot as plt

# Step 1: Create a sample signal with two frequencies (10 Hz and 50 Hz)
fs = 500  # Sampling frequency in Hz
t = np.arange(0, 1, 1/fs)  # Time vector for 1 second duration
f1, f2 = 10, 50  # Define frequencies

# Signal with two frequencies: 10 Hz and 50 Hz
signal = np.sin(2 * np.pi * f1 * t) + 0.5 * np.sin(2 * np.pi * f2 * t)

# Step 2: Perform FFT
n = len(t)  # Number of samples
frequencies = np.fft.fftfreq(n, 1/fs)  # Frequency bins
fft_values = np.fft.fft(signal)  # FFT of the signal

# Step 3: Plot the original signal and its spectrum
plt.figure(figsize=(10, 6))

# Plot time-domain signal
plt.subplot(2, 1, 1)
plt.plot(t, signal)
plt.title("Time-domain Signal")
plt.xlabel("Time [s]")
plt.ylabel("Amplitude")

# Plot frequency-domain (spectrum) signal
plt.subplot(2, 1, 2)
plt.plot(frequencies[:n//2], np.abs(fft_values[:n//2]))  # Plot only the positive frequencies
plt.title("Frequency Spectrum")
plt.xlabel("Frequency [Hz]")
plt.ylabel("Amplitude")

plt.tight_layout()
plt.show()
